---
layout:             post
title:              "Repurposing Protein Folding Models for Generation with Latent Diffusion"
date:               2025-03-26  9:00:00
author:             <a href="https://amyxlu.github.io/">Amy X. Lu</a>
img:                /assets/plaid/main.jpg
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---


<!-- twitter -->
<meta name="twitter:title" content="Repurposing Protein Folding Models for Generation with Latent Diffusion">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://bair.berkeley.edu/static/blog/plaid/main.jpg">

<meta name="keywords" content="Protein Design, Protein Structure Prediction, Latent Diffusion, Multimodal Generation">
<meta name="description" content="The BAIR Blog">
<meta name="author" content="John Doe, Jane Doe">

<!--
The actual text for the post content appears below.  Text will appear on the
homepage, i.e., https://bair.berkeley.edu/blog/ but we only show part of theac
posts on the homepage. The rest is accessed via clicking 'Continue'. This is
enforced with the `more` excerpt separator.
-->

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image1.png" width="50%">
<br>
<i>In PLAID, we sample from the latent space of protein folding models to simultaneously generate protein sequence and all-atom structure, controlled by function and taxonomic prompts.</i>
</p>

The awarding of the 2024 Nobel Prize to AlphaFold2 marks an important moment of recognition for AI's role in biology. What comes next after structure structure prediction?

In PLAID, we develop a method that learns to sample from the latent space of such protein folding models, and can accept compositional function and organism prompts. Unlike previous protein structure generative models, PLAID simultaneously generates both sequence and structure. By using the frozen weights of the pretrained structure predictor, the generative model can be trained on sequence databases 2-4 orders of magnitude larger than structure databases.




<!--more-->



# From protein structure prediction to real-world drug design

Though recent works show promise in the ability for diffusion models to generate proteins, there are many limitations of previous models that make them impractical for real-world applications, such as:

* <span style="color:#17a589">**All-atom generation**</span>: Many existing generative models only produce the backbone atoms. To produce the all-atom structure and place the sidechain atoms, we need to know the sequence. This creates a multimodal generation problem that requires simultaneous generation of discrete and continuous modalities.
* <span style="color:#dc7633">**Organism specificity**</span>: Proteins that are designed to be introduced into the human body need to be humanized, so that the immune system doesn’t destroy the biologic. 
* <span style="color:#9F2B68">**Control specification**</span>: Drug discovery is complex, and has various different constraints. Even after the complex biology is tackled, you might still have logistics-related desiderata (for example, tablets are easier to transport than vials, which might put constraints on solubility). How can we specify these constraints?

# Protein generation using compositional keyword prompts

Can we come up with a compositionally controllable generative interface for protein design? For inspiration, let's consider an interface for controlling image generation, where we specify control via text (example from [Liu et al., 2022][1]).

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image2.png" width="50%">
<br>
<i>Example of compositional text-to-image generation.</i>
</p>


In PLAID, we mirror this interface for <span style="color:#9F2B68">control specification</span> using the <span style="color:#9F2B68">function</span> and the <span style="color:#dc7633">organism</span> for <span style="color:#17a589">all-atom</span> generation:


<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image3.png" width="50%">
<br>
<i>PLAID is able to capture the tetrahedral cysteine-Fe<sup>2+</sup>/Fe<sup>3+</sup> coordination pattern often found in metalloproteins, despite still maintaining high sequence-level diversity.</i>
</p>


# Training using sequence-only training data
**Another important aspect of the PLAID model is that we only require sequences to train the generative model!** Generative models learn the data distribution defined by its training data, and sequence datasets are considerably larger than structural ones, since they are so much cheaper to obtain.


<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image4.png" width="30%">
<br>
<i><b>Figure title.</b> The cost of obtaining protein sequences is much lower than experimentally characterizing structure. As a result, we have much larger sequence databases compared to structural ones, and sequence databases also cover regions of the protein space that don’t form distinct structures.</i>
</p>


# How does it work?
The reason we’re able to train the generative model to generate structure but train it only on sequences is by learning a diffusion model over the latent space of a protein folding model. Then, during inference, after sampling from this latent space of valid proteins, we can take the frozen weights of the protein folding model to obtain the structure. Here, we use ESMFold, a successor to the AlphaFold2 model which replaces a retrieval step with a protein language model.


<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image5.png" width="30%">
<br>
<i><b>Figure title.</b>  The PLAID method during training and inference. ❄️denotes frozen weights.
.</i>
</p>


In this way, we can use structural understanding information in the weights of pretrained protein folding models for the protein design task. This is analogous to how vision-language-action (VLA) models in robotics make use of priors contained in vision-language models (VLMs) trained on internet-scale data to supply perception and reasoning and understanding information.


# Compressing the latent space of protein folding models

A small wrinkle with directly applying this method is that the latent space of ESMFold – indeed, the latent space of many transformer-based models – requires a lot of regularization. This space is also very large, so learning this embedding ends up mapping to high-resolution image synthesis.

To address this, we also propose **CHEAP (Compressed Hourglass Embedding Adaptations of Proteins)**, where we learn a compression model for the joint embedding of protein sequence and structure. 

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image6.png" width="30%">
<br>
<i><b>Figure title.</b> (A) When we visualize the mean value for each channel, some channels exhibit “massive activations”. (B) If we start examining the top-3 activations compared to the median value (gray), we find that this happens over many layers. (C) Massive activations have also been observed for other transformer-based models.</i>
</p>

We find that this latent space is actually highly compressible. By doing a bit of mechanistic interpretability to better understand the base model that we are working with, we were able to create an all-atom protein generative model.



# What's next?

Though we examine the case of protein sequence and structure generation in this work, we can adapt this method to perform multi-modal generation for any modalities where there is a predictor from a more abundant modality to a less abundant one. As sequence-to-structure predictors for proteins are beginning to tackle increasingly complex systems (e.g. AlphaFold3 is also able to predict proteins in complex with nucleic acids and molecular ligands), it’s easy to imagine performing multimodal generation over more complex systems using the same method. 
If you are interested in collaborating to extend our method, or to test our method in the wet-lab, please reach out!

# Further links
If you’ve found our papers useful in your research, please consider using the following BibTeX for PLAID:

```
@article{lu2024generating,
  title={Generating All-Atom Protein Structure from Sequence-Only Training Data},
  author={Lu, Amy X and Yan, Wilson and Robinson, Sarah A and Yang, Kevin K and Gligorijevic, Vladimir and Cho, Kyunghyun and Bonneau, Richard and Abbeel, Pieter and Frey, Nathan},
  journal={bioRxiv},
  pages={2024--12},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}
```

and for CHEAP:

```
@article{lu2024tokenized,
  title={Tokenized and Continuous Embedding Compressions of Protein Sequence and Structure},
  author={Lu, Amy X and Yan, Wilson and Yang, Kevin K and Gligorijevic, Vladimir and Cho, Kyunghyun and Abbeel, Pieter and Bonneau, Richard and Frey, Nathan},
  journal={bioRxiv},
  pages={2024--08},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}
```

You can also checkout our preprints:
* PLAID: [bioRxiv][2]
* CHEAP: [bioRxiv][3]

and codebases:
* PLAID: [GitHub][4]
* CHEAP: [GitHub][5]


# Acknowledgements
Thanks to XYZ for feedback on this draft, and to co-authors across BAIR, Genentech, Microsoft Research, and New York University for their support.

# Additional results

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image7.png" width="30%">
<br>
<i><b>Figure title.</b>  Unconditional generation with PLAID.
</i>

</p>
<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image8.png" width="30%">
<br>
<i><b>Figure title.</b> Additional compositional generations with PLAID.
</i>
</p>

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image9.png" width="30%">
<br>
<i><b>Figure title.</b> Generations prompted by transmembrane keywords place hydrophobic residues at the core, as is biochemically expected.
</i>
</p>

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image10.png" width="30%">
<br>
<i><b>Figure title.</b>  
</i>
</p>

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/plaid/image11.png" width="30%">
<br>
<i><b>Figure title.</b> 
</i>
</p>





[1]:https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/
[2]:https://www.biorxiv.org/content/10.1101/2024.12.02.626353v2
[3]:https://www.biorxiv.org/content/10.1101/2024.08.06.606920v2
[4]:https://github.com/amyxlu/plaid
[5]:https://github.com/amyxlu/cheap-proteins